{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnaolZL7sWIz"
      },
      "source": [
        "### Марковские процессы принятия решений (МППР/MDP [Markov decision process])\n",
        "\n",
        "Вероятности переходов обозначаются $P(s' |s,a)$ и описывают с какой вероятностью вы (агент) окажетесь в состоянии $s'$, если находясь в состоянии  $s$ выберете действие $a$. Есть несколько способов, которыми можно формализовать вознаграждение (reward). В этом ноутбуке мы придерживаемся следующего определения $r(s,a,s')$.\n",
        "\n",
        "_Данный ноутбук честно украден у ШАДа и немного изменён._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ_fvy3tsWI1"
      },
      "source": [
        "Для начала давайте определим вот такую (см. картинку) марковскую цепь:\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/a/ad/Markov_Decision_Process.svg\" width=\"400px\" alt=\"Diagram by Waldoalvarez via Wikimedia Commons, CC BY-SA 4.0\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MG2pbnOhsWI2",
        "outputId": "ac951883-a7aa-4133-de4d-f51c76fdbccc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "(Reading database ... 126455 files and directories currently installed.)\n",
            "Preparing to unpack .../xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.16_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.16) over (2:21.1.4-2ubuntu1.7~22.04.15) ...\n",
            "Preparing to unpack .../xvfb_2%3a21.1.4-2ubuntu1.7~22.04.16_amd64.deb ...\n",
            "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.16) over (2:21.1.4-2ubuntu1.7~22.04.15) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.16) ...\n",
            "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.16) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Starting virtual X frame buffer: Xvfb.\n"
          ]
        }
      ],
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week02_value_based/mdp.py\n",
        "    !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "b4hCk_RzsWI3",
        "outputId": "d7dbe7eb-3e50-4a64-8216-597407be5c6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "transition_probs = {\n",
        "    's0': {\n",
        "        'a0': {'s0': 0.5, 's2': 0.5},\n",
        "        'a1': {'s2': 1}\n",
        "    },\n",
        "    's1': {\n",
        "        'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
        "        'a1': {'s1': 0.95, 's2': 0.05}\n",
        "    },\n",
        "    's2': {\n",
        "        'a0': {'s0': 0.4, 's2': 0.6},\n",
        "        'a1': {'s0': 0.3, 's1': 0.3, 's2': 0.4}\n",
        "    }\n",
        "}\n",
        "rewards = {\n",
        "    's1': {'a0': {'s0': +5}},\n",
        "    's2': {'a1': {'s0': -1}}\n",
        "}\n",
        "\n",
        "from mdp import MDP\n",
        "mdp = MDP(transition_probs, rewards, initial_state='s0')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjuphXwHsWI4"
      },
      "source": [
        "Теперь можно использовать этот MDP как обычный gym env:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QvAHharlsWI5",
        "outputId": "8e1f7e35-40fc-45c1-d491-6f1a487ee193",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial state = s0\n",
            "next_state = s2, reward = 0.0, done = False\n"
          ]
        }
      ],
      "source": [
        "print('initial state =', mdp.reset())\n",
        "next_state, reward, done, info = mdp.step('a1')\n",
        "print('next_state = %s, reward = %s, done = %s' % (next_state, reward, done))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tlxLl9LsWI6"
      },
      "source": [
        "но он также имеет несколько дополнительных методов, которые вам понадобятся"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TORZAMV2sWI6",
        "outputId": "e3631223-a192-495e-af6c-d12187417a86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mdp.get_all_states = ('s0', 's1', 's2')\n",
            "mdp.get_possible_actions('s1') =  ('a0', 'a1')\n",
            "mdp.get_next_states('s1', 'a0') =  {'s0': 0.7, 's1': 0.1, 's2': 0.2}\n",
            "mdp.get_reward('s1', 'a0', 's0') =  5\n",
            "mdp.get_transition_prob('s1', 'a0', 's0') =  0.7\n"
          ]
        }
      ],
      "source": [
        "print(\"mdp.get_all_states =\", mdp.get_all_states())\n",
        "print(\"mdp.get_possible_actions('s1') = \", mdp.get_possible_actions('s1'))\n",
        "print(\"mdp.get_next_states('s1', 'a0') = \", mdp.get_next_states('s1', 'a0'))\n",
        "print(\"mdp.get_reward('s1', 'a0', 's0') = \", mdp.get_reward('s1', 'a0', 's0'))\n",
        "print(\"mdp.get_transition_prob('s1', 'a0', 's0') = \", mdp.get_transition_prob('s1', 'a0', 's0'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qftnj2WKsWI8"
      },
      "source": [
        "### По желанию: Визуализация MDPs\n",
        "\n",
        "Вам нужно установить graphviz\n",
        "\n",
        "1. * Для Ubuntu: `sudo apt-get install graphviz`\n",
        "   * Для OSX: `brew install graphviz`\n",
        "2. `pip install graphviz`\n",
        "3. Перезагрузите ноутбук (Jupyter notebook)\n",
        "\n",
        "__Note:__ На Windows могут быть проблемы с этим пунктом, поэтому в целом, рекомендуем выполнять это задание в Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8FPGTr5xsWI9",
        "outputId": "095f429f-6155-4e61-aa1a-ce4824696e73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graphviz available: True\n"
          ]
        }
      ],
      "source": [
        "from mdp import has_graphviz\n",
        "from IPython.display import display\n",
        "print(\"Graphviz available:\", has_graphviz)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_7QdS2LgsWI9",
        "outputId": "43cda99d-b33b-47f2-a64c-010f2e9dd532",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: MDP Pages: 1 -->\n<svg width=\"1027pt\" height=\"323pt\"\n viewBox=\"0.00 0.00 1027.46 323.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 319)\">\n<title>MDP</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-319 1023.46,-319 1023.46,4 -4,4\"/>\n<!-- s0 -->\n<g id=\"node1\" class=\"node\">\n<title>s0</title>\n<ellipse fill=\"#85ff75\" stroke=\"#85ff75\" cx=\"40\" cy=\"-116\" rx=\"36\" ry=\"36\"/>\n<ellipse fill=\"none\" stroke=\"#85ff75\" cx=\"40\" cy=\"-116\" rx=\"40\" ry=\"40\"/>\n<text text-anchor=\"middle\" x=\"40\" y=\"-109.8\" font-family=\"Arial\" font-size=\"24.00\">s0</text>\n</g>\n<!-- s0&#45;a0 -->\n<g id=\"node2\" class=\"node\">\n<title>s0&#45;a0</title>\n<ellipse fill=\"lightpink\" stroke=\"lightpink\" cx=\"193.58\" cy=\"-160\" rx=\"27.65\" ry=\"27.65\"/>\n<text text-anchor=\"middle\" x=\"193.58\" y=\"-155\" font-family=\"Arial\" font-size=\"20.00\">a0</text>\n</g>\n<!-- s0&#45;&gt;s0&#45;a0 -->\n<g id=\"edge1\" class=\"edge\">\n<title>s0&#45;&gt;s0&#45;a0</title>\n<path fill=\"none\" stroke=\"red\" stroke-width=\"2\" d=\"M79.19,-124.49C99.74,-129.34 125.44,-135.88 148,-143 151.34,-144.05 154.79,-145.22 158.23,-146.44\"/>\n<polygon fill=\"red\" stroke=\"red\" stroke-width=\"2\" points=\"157.17,-149.78 167.77,-149.95 159.59,-143.21 157.17,-149.78\"/>\n</g>\n<!-- s0&#45;a1 -->\n<g id=\"node4\" class=\"node\">\n<title>s0&#45;a1</title>\n<ellipse fill=\"lightpink\" stroke=\"lightpink\" cx=\"193.58\" cy=\"-233\" rx=\"27.65\" ry=\"27.65\"/>\n<text text-anchor=\"middle\" x=\"193.58\" y=\"-228\" font-family=\"Arial\" font-size=\"20.00\">a1</text>\n</g>\n<!-- s0&#45;&gt;s0&#45;a1 -->\n<g id=\"edge4\" class=\"edge\">\n<title>s0&#45;&gt;s0&#45;a1</title>\n<path fill=\"none\" stroke=\"red\" stroke-width=\"2\" d=\"M66.9,-145.7C76.21,-155.57 87.12,-166.26 98,-175 117.23,-190.44 140.89,-204.94 159.73,-215.59\"/>\n<polygon fill=\"red\" stroke=\"red\" stroke-width=\"2\" points=\"158.11,-218.69 168.55,-220.49 161.51,-212.57 158.11,-218.69\"/>\n</g>\n<!-- s0&#45;a0&#45;&gt;s0 -->\n<g id=\"edge2\" class=\"edge\">\n<title>s0&#45;a0&#45;&gt;s0</title>\n<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M166.14,-155.8C146.98,-152.42 120.58,-147.07 98,-140 94.19,-138.81 90.28,-137.45 86.39,-136.01\"/>\n<polygon fill=\"blue\" stroke=\"blue\" points=\"87.39,-132.65 76.8,-132.29 84.85,-139.17 87.39,-132.65\"/>\n<text text-anchor=\"middle\" x=\"123\" y=\"-158.2\" font-family=\"Arial\" font-size=\"16.00\">p = 0.5</text>\n</g>\n<!-- s2 -->\n<g id=\"node3\" class=\"node\">\n<title>s2</title>\n<ellipse fill=\"#85ff75\" stroke=\"#85ff75\" cx=\"433.15\" cy=\"-183\" rx=\"36\" ry=\"36\"/>\n<ellipse fill=\"none\" stroke=\"#85ff75\" cx=\"433.15\" cy=\"-183\" rx=\"40\" ry=\"40\"/>\n<text text-anchor=\"middle\" x=\"433.15\" y=\"-176.8\" font-family=\"Arial\" font-size=\"24.00\">s2</text>\n</g>\n<!-- s0&#45;a0&#45;&gt;s2 -->\n<g id=\"edge3\" class=\"edge\">\n<title>s0&#45;a0&#45;&gt;s2</title>\n<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M221.37,-162.59C260.57,-166.39 334.21,-173.52 383.23,-178.26\"/>\n<polygon fill=\"blue\" stroke=\"blue\" points=\"383,-181.76 393.29,-179.24 383.67,-174.79 383,-181.76\"/>\n<text text-anchor=\"middle\" x=\"307.15\" y=\"-183.2\" font-family=\"Arial\" font-size=\"16.00\">p = 0.5</text>\n</g>\n<!-- s2&#45;a0 -->\n<g id=\"node8\" class=\"node\">\n<title>s2&#45;a0</title>\n<ellipse fill=\"lightpink\" stroke=\"lightpink\" cx=\"666.73\" cy=\"-162\" rx=\"27.65\" ry=\"27.65\"/>\n<text text-anchor=\"middle\" x=\"666.73\" y=\"-157\" font-family=\"Arial\" font-size=\"20.00\">a0</text>\n</g>\n<!-- s2&#45;&gt;s2&#45;a0 -->\n<g id=\"edge13\" class=\"edge\">\n<title>s2&#45;&gt;s2&#45;a0</title>\n<path fill=\"none\" stroke=\"red\" stroke-width=\"2\" d=\"M473.31,-186.98C511.48,-189.83 570.95,-191.53 621.15,-181 624.83,-180.23 628.58,-179.15 632.26,-177.89\"/>\n<polygon fill=\"red\" stroke=\"red\" stroke-width=\"2\" points=\"633.69,-181.09 641.77,-174.24 631.18,-174.56 633.69,-181.09\"/>\n</g>\n<!-- s2&#45;a1 -->\n<g id=\"node9\" class=\"node\">\n<title>s2&#45;a1</title>\n<ellipse fill=\"lightpink\" stroke=\"lightpink\" cx=\"666.73\" cy=\"-80\" rx=\"27.65\" ry=\"27.65\"/>\n<text text-anchor=\"middle\" x=\"666.73\" y=\"-75\" font-family=\"Arial\" font-size=\"20.00\">a1</text>\n</g>\n<!-- s2&#45;&gt;s2&#45;a1 -->\n<g id=\"edge16\" class=\"edge\">\n<title>s2&#45;&gt;s2&#45;a1</title>\n<path fill=\"none\" stroke=\"red\" stroke-width=\"2\" d=\"M467.41,-161.94C475.08,-157.43 483.3,-152.87 491.15,-149 538.2,-125.86 595.06,-104.55 630.82,-91.92\"/>\n<polygon fill=\"red\" stroke=\"red\" stroke-width=\"2\" points=\"632.04,-95.2 640.32,-88.59 629.73,-88.59 632.04,-95.2\"/>\n</g>\n<!-- s0&#45;a1&#45;&gt;s2 -->\n<g id=\"edge5\" class=\"edge\">\n<title>s0&#45;a1&#45;&gt;s2</title>\n<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M221.08,-229.08C256.45,-223.65 320.92,-213 375.15,-200 378.37,-199.23 381.67,-198.39 384.98,-197.5\"/>\n<polygon fill=\"blue\" stroke=\"blue\" points=\"386.21,-200.79 394.91,-194.74 384.33,-194.05 386.21,-200.79\"/>\n<text text-anchor=\"middle\" x=\"307.15\" y=\"-230.2\" font-family=\"Arial\" font-size=\"16.00\">p = 1</text>\n</g>\n<!-- s1 -->\n<g id=\"node5\" class=\"node\">\n<title>s1</title>\n<ellipse fill=\"#85ff75\" stroke=\"#85ff75\" cx=\"829.31\" cy=\"-116\" rx=\"36\" ry=\"36\"/>\n<ellipse fill=\"none\" stroke=\"#85ff75\" cx=\"829.31\" cy=\"-116\" rx=\"40\" ry=\"40\"/>\n<text text-anchor=\"middle\" x=\"829.31\" y=\"-109.8\" font-family=\"Arial\" font-size=\"24.00\">s1</text>\n</g>\n<!-- s1&#45;a0 -->\n<g id=\"node6\" class=\"node\">\n<title>s1&#45;a0</title>\n<ellipse fill=\"lightpink\" stroke=\"lightpink\" cx=\"991.89\" cy=\"-92\" rx=\"27.65\" ry=\"27.65\"/>\n<text text-anchor=\"middle\" x=\"991.89\" y=\"-87\" font-family=\"Arial\" font-size=\"20.00\">a0</text>\n</g>\n<!-- s1&#45;&gt;s1&#45;a0 -->\n<g id=\"edge6\" class=\"edge\">\n<title>s1&#45;&gt;s1&#45;a0</title>\n<path fill=\"none\" stroke=\"red\" stroke-width=\"2\" d=\"M869.38,-112.27C891.95,-109.88 920.82,-106.39 946.31,-102 949,-101.54 951.77,-101.02 954.56,-100.47\"/>\n<polygon fill=\"red\" stroke=\"red\" stroke-width=\"2\" points=\"955.51,-103.84 964.57,-98.36 954.07,-96.99 955.51,-103.84\"/>\n</g>\n<!-- s1&#45;a1 -->\n<g id=\"node7\" class=\"node\">\n<title>s1&#45;a1</title>\n<ellipse fill=\"lightpink\" stroke=\"lightpink\" cx=\"991.89\" cy=\"-174\" rx=\"27.65\" ry=\"27.65\"/>\n<text text-anchor=\"middle\" x=\"991.89\" y=\"-169\" font-family=\"Arial\" font-size=\"20.00\">a1</text>\n</g>\n<!-- s1&#45;&gt;s1&#45;a1 -->\n<g id=\"edge10\" class=\"edge\">\n<title>s1&#45;&gt;s1&#45;a1</title>\n<path fill=\"none\" stroke=\"red\" stroke-width=\"2\" d=\"M867.66,-127.73C890.53,-135.12 920.3,-145.1 946.31,-155 949.82,-156.34 953.46,-157.78 957.08,-159.26\"/>\n<polygon fill=\"red\" stroke=\"red\" stroke-width=\"2\" points=\"955.93,-162.57 966.5,-163.2 958.62,-156.11 955.93,-162.57\"/>\n</g>\n<!-- s1&#45;a0&#45;&gt;s0 -->\n<g id=\"edge7\" class=\"edge\">\n<title>s1&#45;a0&#45;&gt;s0</title>\n<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M972.12,-72.49C944.03,-45.48 888.14,0 830.31,0 192.58,0 192.58,0 192.58,0 141.5,0 96.98,-42.06 69.63,-75.52\"/>\n<polygon fill=\"blue\" stroke=\"blue\" points=\"66.87,-73.37 63.4,-83.38 72.35,-77.72 66.87,-73.37\"/>\n<text text-anchor=\"middle\" x=\"556.15\" y=\"-5.2\" font-family=\"Arial\" font-size=\"16.00\">p = 0.7 &#160;reward =5</text>\n</g>\n<!-- s1&#45;a0&#45;&gt;s2 -->\n<g id=\"edge9\" class=\"edge\">\n<title>s1&#45;a0&#45;&gt;s2</title>\n<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M976.15,-114.86C972.01,-121.81 967.72,-129.58 964.31,-137 953.93,-159.61 966.27,-175.15 946.31,-190 783.96,-310.76 691.37,-229.22 491.15,-200 487.88,-199.52 484.54,-198.9 481.19,-198.17\"/>\n<polygon fill=\"blue\" stroke=\"blue\" points=\"481.76,-194.71 471.21,-195.73 480.1,-201.51 481.76,-194.71\"/>\n<text text-anchor=\"middle\" x=\"741.81\" y=\"-259.2\" font-family=\"Arial\" font-size=\"16.00\">p = 0.2</text>\n</g>\n<!-- s1&#45;a0&#45;&gt;s1 -->\n<g id=\"edge8\" class=\"edge\">\n<title>s1&#45;a0&#45;&gt;s1</title>\n<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M965.31,-83.53C944.06,-77.8 913.22,-72.5 887.31,-80 881.62,-81.65 875.95,-84.05 870.52,-86.84\"/>\n<polygon fill=\"blue\" stroke=\"blue\" points=\"868.63,-83.89 861.64,-91.85 872.07,-89.99 868.63,-83.89\"/>\n<text text-anchor=\"middle\" x=\"916.81\" y=\"-85.2\" font-family=\"Arial\" font-size=\"16.00\">p = 0.1</text>\n</g>\n<!-- s1&#45;a1&#45;&gt;s2 -->\n<g id=\"edge12\" class=\"edge\">\n<title>s1&#45;a1&#45;&gt;s2</title>\n<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M979.29,-198.86C971.84,-211.79 960.81,-226.45 946.31,-234 766.9,-327.45 679.29,-308.33 491.15,-234 482.81,-230.7 474.9,-225.59 467.78,-219.86\"/>\n<polygon fill=\"blue\" stroke=\"blue\" points=\"469.64,-216.84 459.8,-212.91 465.04,-222.12 469.64,-216.84\"/>\n<text text-anchor=\"middle\" x=\"741.81\" y=\"-302.2\" font-family=\"Arial\" font-size=\"16.00\">p = 0.05</text>\n</g>\n<!-- s1&#45;a1&#45;&gt;s1 -->\n<g id=\"edge11\" class=\"edge\">\n<title>s1&#45;a1&#45;&gt;s1</title>\n<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M964.16,-170.97C942.78,-167.92 912.34,-162.18 887.31,-152 881.89,-149.8 876.42,-147.08 871.11,-144.14\"/>\n<polygon fill=\"blue\" stroke=\"blue\" points=\"872.79,-141.06 862.39,-139.02 869.25,-147.1 872.79,-141.06\"/>\n<text text-anchor=\"middle\" x=\"916.81\" y=\"-173.2\" font-family=\"Arial\" font-size=\"16.00\">p = 0.95</text>\n</g>\n<!-- s2&#45;a0&#45;&gt;s0 -->\n<g id=\"edge14\" class=\"edge\">\n<title>s2&#45;a0&#45;&gt;s0</title>\n<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M640,-153.63C633.88,-151.88 627.33,-150.2 621.15,-149 446.29,-114.94 399.16,-130.34 221.15,-123 176.89,-121.17 126.51,-119.23 90.3,-117.85\"/>\n<polygon fill=\"blue\" stroke=\"blue\" points=\"90.14,-114.34 80.01,-117.46 89.87,-121.34 90.14,-114.34\"/>\n<text text-anchor=\"middle\" x=\"307.15\" y=\"-131.2\" font-family=\"Arial\" font-size=\"16.00\">p = 0.4</text>\n</g>\n<!-- s2&#45;a0&#45;&gt;s2 -->\n<g id=\"edge15\" class=\"edge\">\n<title>s2&#45;a0&#45;&gt;s2</title>\n<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M639.38,-158.06C604.93,-153.7 542.86,-148.56 491.15,-159 487.07,-159.82 482.91,-160.95 478.8,-162.27\"/>\n<polygon fill=\"blue\" stroke=\"blue\" points=\"477.59,-158.99 469.34,-165.64 479.93,-165.58 477.59,-158.99\"/>\n<text text-anchor=\"middle\" x=\"556.15\" y=\"-164.2\" font-family=\"Arial\" font-size=\"16.00\">p = 0.6</text>\n</g>\n<!-- s2&#45;a1&#45;&gt;s0 -->\n<g id=\"edge17\" class=\"edge\">\n<title>s2&#45;a1&#45;&gt;s0</title>\n<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M643.56,-64.64C636.68,-60.71 628.88,-57.02 621.15,-55 456.84,-12.07 407.97,-45.54 239.15,-64 175.74,-70.93 158.85,-70.84 98,-90 93.92,-91.29 89.74,-92.79 85.59,-94.42\"/>\n<polygon fill=\"blue\" stroke=\"blue\" points=\"83.93,-91.32 76.04,-98.39 86.62,-97.78 83.93,-91.32\"/>\n<text text-anchor=\"middle\" x=\"307.15\" y=\"-69.2\" font-family=\"Arial\" font-size=\"16.00\">p = 0.3 &#160;reward =&#45;1</text>\n</g>\n<!-- s2&#45;a1&#45;&gt;s2 -->\n<g id=\"edge19\" class=\"edge\">\n<title>s2&#45;a1&#45;&gt;s2</title>\n<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M640.47,-71.31C604.47,-60.59 537.63,-46.98 491.15,-75 469.57,-88.02 455.74,-112.24 447.11,-134.45\"/>\n<polygon fill=\"blue\" stroke=\"blue\" points=\"443.72,-133.54 443.62,-144.14 450.3,-135.92 443.72,-133.54\"/>\n<text text-anchor=\"middle\" x=\"556.15\" y=\"-80.2\" font-family=\"Arial\" font-size=\"16.00\">p = 0.4</text>\n</g>\n<!-- s2&#45;a1&#45;&gt;s1 -->\n<g id=\"edge18\" class=\"edge\">\n<title>s2&#45;a1&#45;&gt;s1</title>\n<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M693.89,-85.86C717.16,-91.08 751.78,-98.84 780.06,-105.18\"/>\n<polygon fill=\"blue\" stroke=\"blue\" points=\"779.64,-108.68 790.16,-107.45 781.17,-101.85 779.64,-108.68\"/>\n<text text-anchor=\"middle\" x=\"741.81\" y=\"-108.2\" font-family=\"Arial\" font-size=\"16.00\">p = 0.3</text>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.graphs.Digraph at 0x798661ecdeb0>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "if has_graphviz:\n",
        "    from mdp import plot_graph, plot_graph_with_state_values, plot_graph_optimal_strategy_and_state_values\n",
        "    display(plot_graph(mdp))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CI4Ei86RsWI-"
      },
      "source": [
        "### Value Iteration\n",
        "\n",
        "Тут начинается суть задания\n",
        "\n",
        "Псевдокод для VI:\n",
        "\n",
        "---\n",
        "\n",
        "`1.` Initialize $V^{(0)}(s)=0$, for all $s$\n",
        "\n",
        "`2.` For $i=0, 1, 2, \\dots$\n",
        "\n",
        "`3.` $ \\quad V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$, for all $s$\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLFSYIn4sWI_"
      },
      "source": [
        "Для начала напишем функцию, вычисляющую $Q^{\\pi}$-функцию, которая определяется следующим образом\n",
        "\n",
        "$$Q_i(s, a) = \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGSRVv_bsWI_"
      },
      "outputs": [],
      "source": [
        "def get_action_value(mdp, state_values, state, action, gamma):\n",
        "    \"\"\" Computes Q(s,a) as in formula above \"\"\"\n",
        "\n",
        "    # <YOUR CODE>\n",
        "\n",
        "    return # <YOUR CODE>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VaPMqAAsWI_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "test_Vs = {s: i for i, s in enumerate(sorted(mdp.get_all_states()))}\n",
        "assert np.isclose(get_action_value(mdp, test_Vs, 's2', 'a1', 0.9), 0.69)\n",
        "assert np.isclose(get_action_value(mdp, test_Vs, 's1', 'a0', 0.9), 3.95)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bty3kJaYsWJA"
      },
      "source": [
        "Использую $Q(s,a)$ мы теперь можем определить \"следующую\"* V(s)-функцию для value iteration.\n",
        " $$V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = \\max_a Q_i(s,a)$$\n",
        "\n",
        " *Следующую в том смысле, что она определена для следующего шага итерации"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPD7tBHIsWJA"
      },
      "outputs": [],
      "source": [
        "def get_new_state_value(mdp, state_values, state, gamma):\n",
        "    \"\"\" Computes next V(s) as in formula above. Please do not change state_values in process. \"\"\"\n",
        "    if mdp.is_terminal(state):\n",
        "        return 0\n",
        "\n",
        "    # <YOUR CODE>\n",
        "\n",
        "    return # <YOUR CODE>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEz-oli9sWJB"
      },
      "outputs": [],
      "source": [
        "test_Vs_copy = dict(test_Vs)\n",
        "assert np.isclose(get_new_state_value(mdp, test_Vs, 's0', 0.9), 1.8)\n",
        "assert np.isclose(get_new_state_value(mdp, test_Vs, 's2', 0.9), 1.08)\n",
        "assert np.isclose(get_new_state_value(mdp, {'s0': -1e10, 's1': 0, 's2': -2e10}, 's0', 0.9), -13500000000.0), \\\n",
        "    \"Please ensure that you handle negative Q-values of arbitrary magnitude correctly\"\n",
        "assert test_Vs == test_Vs_copy, \"Please do not change state_values in get_new_state_value\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ci0yIeWXsWJB"
      },
      "source": [
        "Давайте теперь объединим всё, чтобы получить работающий алгоритм"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4l_ju52XsWJB"
      },
      "outputs": [],
      "source": [
        "# parameters\n",
        "gamma = 0.9            # discount for MDP\n",
        "num_iter = 100         # maximum iterations, excluding initialization\n",
        "# stop VI if new values are this close to old values (or closer)\n",
        "min_difference = 0.001\n",
        "\n",
        "# initialize V(s)\n",
        "state_values = {s: 0 for s in mdp.get_all_states()}\n",
        "\n",
        "if has_graphviz:\n",
        "    display(plot_graph_with_state_values(mdp, state_values))\n",
        "\n",
        "for i in range(num_iter):\n",
        "\n",
        "    # Compute new state values using the functions you defined above.\n",
        "    # It must be a dict {state : float V_new(state)}\n",
        "    new_state_values = <YOUR CODE>\n",
        "\n",
        "    assert isinstance(new_state_values, dict)\n",
        "\n",
        "    # Compute difference\n",
        "    diff = max(abs(new_state_values[s] - state_values[s])\n",
        "               for s in mdp.get_all_states())\n",
        "    print(\"iter %4i   |   diff: %6.5f   |   \" % (i, diff), end=\"\")\n",
        "    print('   '.join(\"V(%s) = %.3f\" % (s, v) for s, v in state_values.items()))\n",
        "    state_values = new_state_values\n",
        "\n",
        "    if diff < min_difference:\n",
        "        print(\"Terminated\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mqi4xOEAsWJC"
      },
      "outputs": [],
      "source": [
        "if has_graphviz:\n",
        "    display(plot_graph_with_state_values(mdp, state_values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZSK5iVQsWJC"
      },
      "outputs": [],
      "source": [
        "print(\"Final state values:\", state_values)\n",
        "\n",
        "assert abs(state_values['s0'] - 3.781) < 0.01\n",
        "assert abs(state_values['s1'] - 7.294) < 0.01\n",
        "assert abs(state_values['s2'] - 4.202) < 0.01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n97ODvZ5sWJC"
      },
      "source": [
        "Теперь, когда мы нашли оптимальную $V^{*}(s)$ (звёздочка означает, что она оптимальная), мы можем \"вытащить\" оптимальные действия для каждого состояния\n",
        "\n",
        " $$\\pi^*(s) = argmax_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = argmax_a Q_i(s,a)$$\n",
        "\n",
        "В чём разница с нахождением $V^{*}(s)$, которое вы сделали выше? Теперь тут стоит не max, а argmax. То есть мы \"вытаскиваем\" такие действия, которые приводят к максимальной Q(s,a)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-d1hSQlsWJC"
      },
      "outputs": [],
      "source": [
        "def get_optimal_action(mdp, state_values, state, gamma=0.9):\n",
        "    \"\"\" Finds optimal action using formula above. \"\"\"\n",
        "    if mdp.is_terminal(state):\n",
        "        return None\n",
        "\n",
        "    <YOUR CODE>\n",
        "\n",
        "    return <YOUR CODE>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kb1pUQyVsWJD"
      },
      "outputs": [],
      "source": [
        "assert get_optimal_action(mdp, state_values, 's0', gamma) == 'a1'\n",
        "assert get_optimal_action(mdp, state_values, 's1', gamma) == 'a0'\n",
        "assert get_optimal_action(mdp, state_values, 's2', gamma) == 'a1'\n",
        "\n",
        "assert get_optimal_action(mdp, {'s0': -1e10, 's1': 0, 's2': -2e10}, 's0', 0.9) == 'a0', \\\n",
        "    \"Please ensure that you handle negative Q-values of arbitrary magnitude correctly\"\n",
        "assert get_optimal_action(mdp, {'s0': -2e10, 's1': 0, 's2': -1e10}, 's0', 0.9) == 'a1', \\\n",
        "    \"Please ensure that you handle negative Q-values of arbitrary magnitude correctly\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2rWr9p5sWJD"
      },
      "outputs": [],
      "source": [
        "if has_graphviz:\n",
        "    display(plot_graph_optimal_strategy_and_state_values(mdp, state_values, get_action_value))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhy9D1KnsWJD"
      },
      "outputs": [],
      "source": [
        "# Measure agent's average reward\n",
        "\n",
        "s = mdp.reset()\n",
        "rewards = []\n",
        "for _ in range(10000):\n",
        "    s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, gamma))\n",
        "    rewards.append(r)\n",
        "\n",
        "print(\"average reward: \", np.mean(rewards))\n",
        "\n",
        "assert(0.40 < np.mean(rewards) < 0.55)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZGQScN_sWJD"
      },
      "source": [
        "### Frozen lake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8IZhxKzsWJE"
      },
      "outputs": [],
      "source": [
        "from mdp import FrozenLakeEnv\n",
        "mdp = FrozenLakeEnv(slip_chance=0)\n",
        "\n",
        "mdp.render()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FuVxGo2sWJE"
      },
      "outputs": [],
      "source": [
        "def value_iteration(mdp, state_values=None, gamma=0.9, num_iter=1000, min_difference=1e-5):\n",
        "    \"\"\" performs num_iter value iteration steps starting from state_values. Same as before but in a function \"\"\"\n",
        "    state_values = state_values or {s: 0 for s in mdp.get_all_states()}\n",
        "    for i in range(num_iter):\n",
        "\n",
        "        # Compute new state values using the functions you defined above. It must be a dict {state : new_V(state)}\n",
        "        new_state_values = <YOUR CODE>\n",
        "\n",
        "        assert isinstance(new_state_values, dict)\n",
        "\n",
        "        # Compute difference\n",
        "        diff = max(abs(new_state_values[s] - state_values[s])\n",
        "                   for s in mdp.get_all_states())\n",
        "\n",
        "        print(\"iter %4i   |   diff: %6.5f   |   V(start): %.3f \" %\n",
        "              (i, diff, new_state_values[mdp._initial_state]))\n",
        "\n",
        "        state_values = new_state_values\n",
        "        if diff < min_difference:\n",
        "            break\n",
        "\n",
        "    return state_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuV3AwPfsWJE"
      },
      "outputs": [],
      "source": [
        "state_values = value_iteration(mdp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HE5YlPORsWJK"
      },
      "outputs": [],
      "source": [
        "s = mdp.reset()\n",
        "mdp.render()\n",
        "for t in range(100):\n",
        "    a = get_optimal_action(mdp, state_values, s, gamma)\n",
        "    print(a, end='\\n\\n')\n",
        "    s, r, done, _ = mdp.step(a)\n",
        "    mdp.render()\n",
        "    if done:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "um9PNgSBsWJL"
      },
      "source": [
        "### Давайте порисуем картинки!\n",
        "\n",
        "Чтобы наглядно посмотреть как учится агент, мы будем \"рисовать\" значения $V(s)$ и стрелочки оптимальных действий для каждой итерации VI.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTp1zyZdsWJL"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "def draw_policy(mdp, state_values):\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    h, w = mdp.desc.shape\n",
        "    states = sorted(mdp.get_all_states())\n",
        "    V = np.array([state_values[s] for s in states])\n",
        "    Pi = {s: get_optimal_action(mdp, state_values, s, gamma) for s in states}\n",
        "    plt.imshow(V.reshape(w, h), cmap='gray', interpolation='none', clim=(0, 1))\n",
        "    ax = plt.gca()\n",
        "    ax.set_xticks(np.arange(h)-.5)\n",
        "    ax.set_yticks(np.arange(w)-.5)\n",
        "    ax.set_xticklabels([])\n",
        "    ax.set_yticklabels([])\n",
        "    Y, X = np.mgrid[0:4, 0:4]\n",
        "    a2uv = {'left': (-1, 0), 'down': (0, -1), 'right': (1, 0), 'up': (0, 1)}\n",
        "    for y in range(h):\n",
        "        for x in range(w):\n",
        "            plt.text(x, y, str(mdp.desc[y, x].item()),\n",
        "                     color='g', size=12,  verticalalignment='center',\n",
        "                     horizontalalignment='center', fontweight='bold')\n",
        "            a = Pi[y, x]\n",
        "            if a is None:\n",
        "                continue\n",
        "            u, v = a2uv[a]\n",
        "            plt.arrow(x, y, u*.3, -v*.3, color='m',\n",
        "                      head_width=0.1, head_length=0.1)\n",
        "    plt.grid(color='b', lw=2, ls='-')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6u-Wx0MysWJM"
      },
      "outputs": [],
      "source": [
        "state_values = {s: 0 for s in mdp.get_all_states()}\n",
        "\n",
        "for i in range(10):\n",
        "    print(\"after iteration %i\" % i)\n",
        "    state_values = value_iteration(mdp, state_values, num_iter=1)\n",
        "    draw_policy(mdp, state_values)\n",
        "# please ignore iter 0 at each step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfZkOPctsWJM"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "mdp = FrozenLakeEnv(map_name='8x8', slip_chance=0.1)\n",
        "state_values = {s: 0 for s in mdp.get_all_states()}\n",
        "\n",
        "for i in range(30):\n",
        "    clear_output(True)\n",
        "    print(\"after iteration %i\" % i)\n",
        "    state_values = value_iteration(mdp, state_values, num_iter=1)\n",
        "    draw_policy(mdp, state_values)\n",
        "    sleep(0.5)\n",
        "# please ignore iter 0 at each step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtgu8t9ksWJM"
      },
      "source": [
        "Доп тесты"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aic4e2i5sWJN"
      },
      "outputs": [],
      "source": [
        "mdp = FrozenLakeEnv(slip_chance=0)\n",
        "state_values = value_iteration(mdp)\n",
        "\n",
        "total_rewards = []\n",
        "for game_i in range(1000):\n",
        "    s = mdp.reset()\n",
        "    rewards = []\n",
        "    for t in range(100):\n",
        "        s, r, done, _ = mdp.step(\n",
        "            get_optimal_action(mdp, state_values, s, gamma))\n",
        "        rewards.append(r)\n",
        "        if done:\n",
        "            break\n",
        "    total_rewards.append(np.sum(rewards))\n",
        "\n",
        "print(\"average reward: \", np.mean(total_rewards))\n",
        "assert(1.0 <= np.mean(total_rewards) <= 1.0)\n",
        "print(\"Well done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfJnk0kvsWJN"
      },
      "outputs": [],
      "source": [
        "# Measure agent's average reward\n",
        "mdp = FrozenLakeEnv(slip_chance=0.1)\n",
        "state_values = value_iteration(mdp)\n",
        "\n",
        "total_rewards = []\n",
        "for game_i in range(1000):\n",
        "    s = mdp.reset()\n",
        "    rewards = []\n",
        "    for t in range(100):\n",
        "        s, r, done, _ = mdp.step(\n",
        "            get_optimal_action(mdp, state_values, s, gamma))\n",
        "        rewards.append(r)\n",
        "        if done:\n",
        "            break\n",
        "    total_rewards.append(np.sum(rewards))\n",
        "\n",
        "print(\"average reward: \", np.mean(total_rewards))\n",
        "assert(0.8 <= np.mean(total_rewards) <= 0.95)\n",
        "print(\"Well done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vZVB42TsWJO"
      },
      "outputs": [],
      "source": [
        "# Measure agent's average reward\n",
        "mdp = FrozenLakeEnv(slip_chance=0.25)\n",
        "state_values = value_iteration(mdp)\n",
        "\n",
        "total_rewards = []\n",
        "for game_i in range(1000):\n",
        "    s = mdp.reset()\n",
        "    rewards = []\n",
        "    for t in range(100):\n",
        "        s, r, done, _ = mdp.step(\n",
        "            get_optimal_action(mdp, state_values, s, gamma))\n",
        "        rewards.append(r)\n",
        "        if done:\n",
        "            break\n",
        "    total_rewards.append(np.sum(rewards))\n",
        "\n",
        "print(\"average reward: \", np.mean(total_rewards))\n",
        "assert(0.6 <= np.mean(total_rewards) <= 0.7)\n",
        "print(\"Well done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PkEtrKIsWJP"
      },
      "outputs": [],
      "source": [
        "# Measure agent's average reward\n",
        "mdp = FrozenLakeEnv(slip_chance=0.2, map_name='8x8')\n",
        "state_values = value_iteration(mdp)\n",
        "\n",
        "total_rewards = []\n",
        "for game_i in range(1000):\n",
        "    s = mdp.reset()\n",
        "    rewards = []\n",
        "    for t in range(100):\n",
        "        s, r, done, _ = mdp.step(\n",
        "            get_optimal_action(mdp, state_values, s, gamma))\n",
        "        rewards.append(r)\n",
        "        if done:\n",
        "            break\n",
        "    total_rewards.append(np.sum(rewards))\n",
        "\n",
        "print(\"average reward: \", np.mean(total_rewards))\n",
        "assert(0.6 <= np.mean(total_rewards) <= 0.8)\n",
        "print(\"Well done!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}